{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/data.txt\", \"r\") as f:\n",
    "\tdata = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(text):\n",
    "\tcleaned_text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n",
    "\tcleaned_text = re.sub(r\"\\s+\", \" \", cleaned_text)\n",
    "\tcleaned_text = cleaned_text.lower()\n",
    "\ttokens = cleaned_text.split(\" \")\n",
    "\twith open(\"../data/stopwords-en.txt\", \"r\") as f:\n",
    "\t\tstop_words = f.read()\n",
    "\tstop_words = stop_words.replace(\"\\n\", \" \").split(\" \")\n",
    "\treturn [token for token in tokens if token not in stop_words[:-1]]\n",
    "\n",
    "tokens = clean_and_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = set(tokens)\n",
    "word_i = {word: i for (i, word) in enumerate(unique_words)}\n",
    "i_word = {i: word for (i, word) in enumerate(unique_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "\n",
    "\n",
    "def target_context_tuples(tokens, window_size):\n",
    "\tcontext = []\n",
    "\tfor i, token in enumerate(tokens):\n",
    "\t\tcontext_words = [t for t in merge(tokens, i, window_size) if t != token]\n",
    "\t\tfor c in context_words:\n",
    "\t\t\tcontext.append((token, c))\n",
    "\treturn context\n",
    "\n",
    "\n",
    "def merge(tokens, i, window_size):\n",
    "\tleft_id = i - window_size if i >= window_size else i - 1 if i != 0 else i\n",
    "\tright_id = i + window_size + 1 if i + window_size <= len(tokens) else len(tokens)\n",
    "\treturn tokens[left_id:right_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deep', 'learning'),\n",
       " ('deep', 'subset'),\n",
       " ('learning', 'deep'),\n",
       " ('learning', 'subset'),\n",
       " ('learning', 'machine'),\n",
       " ('subset', 'deep'),\n",
       " ('subset', 'learning'),\n",
       " ('subset', 'machine'),\n",
       " ('subset', 'learning'),\n",
       " ('machine', 'learning'),\n",
       " ('machine', 'subset'),\n",
       " ('machine', 'learning'),\n",
       " ('machine', 'methods'),\n",
       " ('learning', 'subset'),\n",
       " ('learning', 'machine'),\n",
       " ('learning', 'methods'),\n",
       " ('learning', 'based'),\n",
       " ('methods', 'machine'),\n",
       " ('methods', 'learning'),\n",
       " ('methods', 'based')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_context_pairs = target_context_tuples(tokens, 2)\n",
    "target_context_pairs[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(target_context_pairs, columns=[\"target\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "vocab_size = len(unique_words)\n",
    "token_indexes = [word_i[token] for token in unique_words]\n",
    "encodings = F.one_hot(torch.tensor(token_indexes), num_classes=vocab_size).float()\n",
    "\n",
    "df[\"target_ohe\"] = df[\"target\"].apply(lambda x: encodings[word_i[x]])\n",
    "df[\"context_ohe\"] = df[\"context\"].apply(lambda x: encodings[word_i[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>target_ohe</th>\n",
       "      <th>context_ohe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deep</td>\n",
       "      <td>learning</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deep</td>\n",
       "      <td>subset</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>learning</td>\n",
       "      <td>deep</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>learning</td>\n",
       "      <td>subset</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>learning</td>\n",
       "      <td>machine</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "      <td>[tensor(0.), tensor(0.), tensor(0.), tensor(0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target   context                                         target_ohe  \\\n",
       "0      deep  learning  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "1      deep    subset  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "2  learning      deep  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "3  learning    subset  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "4  learning   machine  [tensor(0.), tensor(0.), tensor(0.), tensor(0....   \n",
       "\n",
       "                                         context_ohe  \n",
       "0  [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "1  [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "2  [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "3  [tensor(0.), tensor(0.), tensor(0.), tensor(0....  \n",
       "4  [tensor(0.), tensor(0.), tensor(0.), tensor(0....  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class W2VDataset(Dataset):\n",
    "\tdef __init__(self, df):\n",
    "\t\tself.df = df\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.df)\n",
    "\t\n",
    "\tdef __getitem__(self, index):\n",
    "\t\tcontext = df[\"context_ohe\"][index]\n",
    "\t\ttarget = df[\"target_ohe\"][index]\n",
    "\t\treturn context, target\n",
    "\n",
    "\tdataset = W2VDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(torch.nn.Module):\n",
    "\tdef __init__(self, vocab_size, embed_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.linear_1 = torch.nn.Linear(vocab_size, embed_size)\n",
    "\t\tself.linear_2 = torch.nn.Linear(embed_size, vocab_size, bias=False)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.linear_1(x)\n",
    "\t\tx = self.linear_2(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "EMBED_SIZE = 10\n",
    "model = Word2Vec(vocab_size, EMBED_SIZE).to(device)\n",
    "learning_rate = 1e-2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 3.6006102107820057\n",
      "Epoch: 20 | Loss: 2.7755035161972046\n",
      "Epoch: 30 | Loss: 2.5514752126875377\n",
      "Epoch: 40 | Loss: 2.451943897065662\n",
      "Epoch: 50 | Loss: 2.374846617380778\n",
      "Epoch: 60 | Loss: 2.3378889447166804\n",
      "Epoch: 70 | Loss: 2.2985056212970187\n",
      "Epoch: 80 | Loss: 2.2799387602579024\n",
      "Epoch: 90 | Loss: 2.2576661989802407\n",
      "Epoch: 100 | Loss: 2.2484866692906333\n",
      "Epoch: 110 | Loss: 2.230420458884466\n",
      "Epoch: 120 | Loss: 2.222761889298757\n",
      "Epoch: 130 | Loss: 2.212623025689806\n",
      "Epoch: 140 | Loss: 2.205689932618822\n",
      "Epoch: 150 | Loss: 2.202955722808838\n",
      "Epoch: 160 | Loss: 2.1946745089122226\n",
      "Epoch: 170 | Loss: 2.2002311746279397\n",
      "Epoch: 180 | Loss: 2.1902154116403487\n",
      "Epoch: 190 | Loss: 2.1893660511289323\n",
      "Epoch: 200 | Loss: 2.17938608782632\n",
      "Epoch: 210 | Loss: 2.183610717455546\n",
      "Epoch: 220 | Loss: 2.1823562525567555\n",
      "Epoch: 230 | Loss: 2.1823100447654724\n",
      "Epoch: 240 | Loss: 2.173426369825999\n",
      "Epoch: 250 | Loss: 2.17266746645882\n",
      "Epoch: 260 | Loss: 2.1697739646548317\n",
      "Epoch: 270 | Loss: 2.1730494385673884\n",
      "Epoch: 280 | Loss: 2.164461448079064\n",
      "Epoch: 290 | Loss: 2.173994881766183\n",
      "Epoch: 300 | Loss: 2.1606431944029674\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\trunning_loss = 0.0\n",
    "\tfor batch, (context, target) in enumerate(dataloader):\n",
    "\t\tcontext = context.to(device)\n",
    "\t\ttarget = target.to(device)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tpred = model(context)\n",
    "\t\tloss = criterion(pred, target)\n",
    "\t\trunning_loss += loss.item()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\tepoch_loss = running_loss / len(dataloader)\n",
    "\tif (epoch + 1) % 10 == 0:\n",
    "\t\tprint(f\"Epoch: {epoch+1} | Loss: {epoch_loss}\")\n",
    "\n",
    "\tloss_history.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['processing', 'machine', 'natural', 'recognition', 'language']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = encodings[word_i[\"language\"]]\n",
    "[i_word[i.item()] for i in torch.argsort(model(word.to(device)), descending=True).squeeze(0)[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['organisms', 'study', 'various', 'emerged', 'energy']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = encodings[word_i[\"life\"]]\n",
    "[i_word[i.item()] for i in torch.argsort(model(word.to(device)), descending=True).squeeze(0)[:5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7124,  0.9761, -1.3027, -1.3146,  1.7716, -0.3412,  2.5923, -1.2782,\n",
       "        -1.5474, -0.8365])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word_embedding(model, word):\n",
    "\tembeddings = model.linear_2.weight.detach().cpu()\n",
    "\tid = word_i[word]\n",
    "\treturn embeddings[id]\n",
    "\n",
    "get_word_embedding(model, \"biology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
